{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc712d2-8b62-4098-a986-76ec60c795c4",
   "metadata": {},
   "source": [
    "# Lesson 7: Adding Prompt & Resource Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42777ce6-c826-4f1a-909f-f31aa9799e99",
   "metadata": {},
   "source": [
    "In the previous lessons, you created an MCP server that provides only tools. In this lesson, you are provided with an updated file for the research server file which now provides a prompt template and 2 resources features in addition to the 2 tools. You are also provided with an updated file for the mcp chatbot file where the MCP client exposes the prompt and resources for the user to use. The files are provided in the `mcp_project` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818369d9-0001-46bf-ae95-9a6fb0d5977a",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> ðŸ’» &nbsp; <b> To Access the  <code>mcp_project</code> folder :</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em> and finally 3) click on <em>L7</em>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f59fc7",
   "metadata": {},
   "source": [
    "## Defining Resources and Prompts in the MCP Server - Optional Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20132d65-49f8-4b06-a122-e19d01aeceb1",
   "metadata": {},
   "source": [
    "Feel to read this section before or after you watch the video. You can always skip and go to the end of the notebook to run the updated chatbot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11fbf8-4620-4eec-9d8b-6cd3158143ed",
   "metadata": {},
   "source": [
    "**Resources**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dfa65e-1a60-4f64-abb5-597d0145652d",
   "metadata": {},
   "source": [
    "You learned from lesson 3 that the research server saves the information of the researched papers in a `json` file called `papers_info.json`, which is stored under a folder labeled with the topic name. All the topics are stored under the `papers` directory. If you check the `papers` folder provided to you under `mcp_project`, you will find two folders labeled `ai_interpretability` and `llm_reasoning`, and in each folder, you have `papers_info.json` file. \n",
    "\n",
    "Resources are read-only data that an MCP server can expose to the LLM application. Resources are similar to GET endpoints in a REST API - they provide data but shouldn't perform significant computation or have side effects. For example, the resource can be a list of folders within a directory or the content of a file within a folder. Here, the MCP server provides two resources:\n",
    "- list of available topic folders under the papers directory;\n",
    "- the papers' information stored under a topic folder.\n",
    "\n",
    "Here's a code snippet that shows how resources are defined in the MCP server again using `FastMCP` (with the use of the decorator `@mcp.resource(uri)`). You can find the complete code in the `mcp_project` folder. The URI defined inside `@mcp.resource()` is used to uniquely identify the resource and, as a server developer, you can customize the URI. But in general, it follows this scheme:\n",
    "`sth://xyz/xcv` . In this example, two types of URI were used:\n",
    "- static URI: `papers://folders` (which represents the list of available topics)\n",
    "- dynamic URI: `papers://{topic}` (which represents the papers' information under the topic specified by the client during runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c25206-76d6-4449-8af7-b41209fbbdfc",
   "metadata": {},
   "source": [
    "``` python\n",
    "@mcp.resource(\"papers://folders\")\n",
    "def get_available_folders() -> str:\n",
    "    \"\"\"\n",
    "    List all available topic folders in the papers directory.\n",
    "    \n",
    "    This resource provides a simple list of all available topic folders.\n",
    "    \"\"\"\n",
    "    folders = []\n",
    "    \n",
    "    # Get all topic directories\n",
    "    if os.path.exists(PAPER_DIR):\n",
    "        for topic_dir in os.listdir(PAPER_DIR):\n",
    "            topic_path = os.path.join(PAPER_DIR, topic_dir)\n",
    "            if os.path.isdir(topic_path):\n",
    "                papers_file = os.path.join(topic_path, \"papers_info.json\")\n",
    "                if os.path.exists(papers_file):\n",
    "                    folders.append(topic_dir)\n",
    "    \n",
    "    # Create a simple markdown list\n",
    "    content = \"# Available Topics\\n\\n\"\n",
    "    if folders:\n",
    "        for folder in folders:\n",
    "            content += f\"- {folder}\\n\"\n",
    "        content += f\"\\nUse @{folder} to access papers in that topic.\\n\"\n",
    "    else:\n",
    "        content += \"No topics found.\\n\"\n",
    "    \n",
    "    return content\n",
    "\n",
    "@mcp.resource(\"papers://{topic}\")\n",
    "def get_topic_papers(topic: str) -> str:\n",
    "    \"\"\"\n",
    "    Get detailed information about papers on a specific topic.\n",
    "    \n",
    "    Args:\n",
    "        topic: The research topic to retrieve papers for\n",
    "    \"\"\"\n",
    "    topic_dir = topic.lower().replace(\" \", \"_\")\n",
    "    papers_file = os.path.join(PAPER_DIR, topic_dir, \"papers_info.json\")\n",
    "    \n",
    "    if not os.path.exists(papers_file):\n",
    "        return f\"# No papers found for topic: {topic}\\n\\nTry searching for papers on this topic first.\"\n",
    "    \n",
    "    try:\n",
    "        with open(papers_file, 'r') as f:\n",
    "            papers_data = json.load(f)\n",
    "        \n",
    "        # Create markdown content with paper details\n",
    "        content = f\"# Papers on {topic.replace('_', ' ').title()}\\n\\n\"\n",
    "        content += f\"Total papers: {len(papers_data)}\\n\\n\"\n",
    "        \n",
    "        for paper_id, paper_info in papers_data.items():\n",
    "            content += f\"## {paper_info['title']}\\n\"\n",
    "            content += f\"- **Paper ID**: {paper_id}\\n\"\n",
    "            content += f\"- **Authors**: {', '.join(paper_info['authors'])}\\n\"\n",
    "            content += f\"- **Published**: {paper_info['published']}\\n\"\n",
    "            content += f\"- **PDF URL**: [{paper_info['pdf_url']}]({paper_info['pdf_url']})\\n\\n\"\n",
    "            content += f\"### Summary\\n{paper_info['summary'][:500]}...\\n\\n\"\n",
    "            content += \"---\\n\\n\"\n",
    "        \n",
    "        return content\n",
    "    except json.JSONDecodeError:\n",
    "        return f\"# Error reading papers data for {topic}\\n\\nThe papers data file is corrupted.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623bfcd9-a6db-43f7-a269-7d04bea1f0e6",
   "metadata": {},
   "source": [
    "**Prompt Template**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b264d48-0e5d-4516-b9f3-3c320ce68c68",
   "metadata": {},
   "source": [
    "Server can also provide a prompt template. You can define this feature in the MCP server using the decorator `@mcp.prompt()` as shown in the code snippet below. MCP will use `generate_search_prompt` as the prompt name, infer the prompt arguments from the function's argument and the prompt's description from the doc string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe4dea-14a3-4c80-8303-a29e52111e3d",
   "metadata": {},
   "source": [
    "```python\n",
    "@mcp.prompt()\n",
    "def generate_search_prompt(topic: str, num_papers: int = 5) -> str:\n",
    "    \"\"\"Generate a prompt for Claude to find and discuss academic papers on a specific topic.\"\"\"\n",
    "    return f\"\"\"Search for {num_papers} academic papers about '{topic}' using the search_papers tool. Follow these instructions:\n",
    "    1. First, search for papers using search_papers(topic='{topic}', max_results={num_papers})\n",
    "    2. For each paper found, extract and organize the following information:\n",
    "       - Paper title\n",
    "       - Authors\n",
    "       - Publication date\n",
    "       - Brief summary of the key findings\n",
    "       - Main contributions or innovations\n",
    "       - Methodologies used\n",
    "       - Relevance to the topic '{topic}'\n",
    "    \n",
    "    3. Provide a comprehensive summary that includes:\n",
    "       - Overview of the current state of research in '{topic}'\n",
    "       - Common themes and trends across the papers\n",
    "       - Key research gaps or areas for future investigation\n",
    "       - Most impactful or influential papers in this area\n",
    "    \n",
    "    4. Organize your findings in a clear, structured format with headings and bullet points for easy readability.\n",
    "    \n",
    "    Please present both detailed information about each paper and a high-level synthesis of the research landscape in {topic}.\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66606464-9079-4171-9999-7f3f1a3b20e4",
   "metadata": {},
   "source": [
    "## Using Resources and Prompts in the MCP Chatbot - Optional Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2799c8",
   "metadata": {},
   "source": [
    "The chatbot is updated to enable users to interact with the prompt using the slash command:\n",
    "- Users can use the command `/prompts` to list the available prompts\n",
    "- Users can use the command `/prompt <name> <arg1=value1>` to invoke a particular prompt\n",
    "  \n",
    "The chatbot also enables users to interact with the resources using the `@` character:\n",
    "- Users can use the command `@folders` to see available topics\n",
    "- Users can use the command `@topic` to get papers info under that topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849912ad-77ac-40b7-b78a-383bb998921c",
   "metadata": {},
   "source": [
    "Make sure to check the updated code in the `mcp_project` folder. There's a couple of newly added methods (`get_resource`, `execute_prompt`, `list_prompts`). Here's a brief summary of the updates added to the chatbot:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48921da",
   "metadata": {},
   "source": [
    "- In `connect_to_server`: the client requests from the server to list the resources and prompt templates they provide (in addition to the tools request). The resource URIs and the prompt names are stored in the MCP chatbot.\n",
    "   \n",
    "    <img src=\"images/resource_discovery.png\" width=\"400\">\n",
    "\n",
    "\n",
    "    <img src=\"images/prompt_discovery.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a540566d",
   "metadata": {},
   "source": [
    "- In `chat_loop`: the user's input is checked to see if the user has used any of the slash commands or @ options.\n",
    "\n",
    "- If the user types: `@folders` or `@topic` then the newly added method `get_resource` is called where the request is sent to the server.\n",
    "   \n",
    "    <img src=\"images/resource_invocation.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc03efc",
   "metadata": {},
   "source": [
    "- If the user types: `/prompts`, then the newly added method `list_prompts` is called.\n",
    "   \n",
    "- If the user types: `/prompt <name> <arg1=value1>`, then the newly added method `execute_prompt` is called where the request is sent to the server:\n",
    "   \n",
    "   <img src=\"images/prompt_invocation.png\" width=\"400\">\n",
    "   \n",
    "   and the prompt is passed to the LLM. \n",
    "- Otherwise the query is processed by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e3e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from contextlib import AsyncExitStack\n",
    "import json\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class MCP_ChatBot:\n",
    "    def __init__(self):\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.anthropic = Anthropic()\n",
    "        # Tools list required for Anthropic API\n",
    "        self.available_tools = []\n",
    "        # Prompts list for quick display \n",
    "        self.available_prompts = []\n",
    "        # Sessions dict maps tool/prompt names or resource URIs to MCP client sessions\n",
    "        self.sessions = {}\n",
    "\n",
    "    async def connect_to_server(self, server_name, server_config):\n",
    "        try:\n",
    "            server_params = StdioServerParameters(**server_config)\n",
    "            stdio_transport = await self.exit_stack.enter_async_context(\n",
    "                stdio_client(server_params)\n",
    "            )\n",
    "            read, write = stdio_transport\n",
    "            session = await self.exit_stack.enter_async_context(\n",
    "                ClientSession(read, write)\n",
    "            )\n",
    "            await session.initialize()\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                # List available tools\n",
    "                response = await session.list_tools()\n",
    "                for tool in response.tools:\n",
    "                    self.sessions[tool.name] = session\n",
    "                    self.available_tools.append({\n",
    "                        \"name\": tool.name,\n",
    "                        \"description\": tool.description,\n",
    "                        \"input_schema\": tool.inputSchema\n",
    "                    })\n",
    "            \n",
    "                # List available prompts\n",
    "                prompts_response = await session.list_prompts()\n",
    "                if prompts_response and prompts_response.prompts:\n",
    "                    for prompt in prompts_response.prompts:\n",
    "                        self.sessions[prompt.name] = session\n",
    "                        self.available_prompts.append({\n",
    "                            \"name\": prompt.name,\n",
    "                            \"description\": prompt.description,\n",
    "                            \"arguments\": prompt.arguments\n",
    "                        })\n",
    "                # List available resources\n",
    "                resources_response = await session.list_resources()\n",
    "                if resources_response and resources_response.resources:\n",
    "                    for resource in resources_response.resources:\n",
    "                        resource_uri = str(resource.uri)\n",
    "                        self.sessions[resource_uri] = session\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to {server_name}: {e}\")\n",
    "\n",
    "    async def connect_to_servers(self):\n",
    "        try:\n",
    "            with open(\"server_config.json\", \"r\") as file:\n",
    "                data = json.load(file)\n",
    "            servers = data.get(\"mcpServers\", {})\n",
    "            for server_name, server_config in servers.items():\n",
    "                await self.connect_to_server(server_name, server_config)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading server config: {e}\")\n",
    "            raise\n",
    "    \n",
    "    async def process_query(self, query):\n",
    "        messages = [{'role':'user', 'content':query}]\n",
    "        \n",
    "        while True:\n",
    "            response = self.anthropic.messages.create(\n",
    "                max_tokens = 2024,\n",
    "                model = 'claude-3-7-sonnet-20250219', \n",
    "                tools = self.available_tools,\n",
    "                messages = messages\n",
    "            )\n",
    "            \n",
    "            assistant_content = []\n",
    "            has_tool_use = False\n",
    "            \n",
    "            for content in response.content:\n",
    "                if content.type == 'text':\n",
    "                    print(content.text)\n",
    "                    assistant_content.append(content)\n",
    "                elif content.type == 'tool_use':\n",
    "                    has_tool_use = True\n",
    "                    assistant_content.append(content)\n",
    "                    messages.append({'role':'assistant', 'content':assistant_content})\n",
    "                    \n",
    "                    # Get session and call tool\n",
    "                    session = self.sessions.get(content.name)\n",
    "                    if not session:\n",
    "                        print(f\"Tool '{content.name}' not found.\")\n",
    "                        break\n",
    "                        \n",
    "                    result = await session.call_tool(content.name, arguments=content.input)\n",
    "                    messages.append({\n",
    "                        \"role\": \"user\", \n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"tool_result\",\n",
    "                                \"tool_use_id\": content.id,\n",
    "                                \"content\": result.content\n",
    "                            }\n",
    "                        ]\n",
    "                    })\n",
    "            \n",
    "            # Exit loop if no tool was used\n",
    "            if not has_tool_use:\n",
    "                break\n",
    "\n",
    "    async def get_resource(self, resource_uri):\n",
    "        session = self.sessions.get(resource_uri)\n",
    "        \n",
    "        # Fallback for papers URIs - try any papers resource session\n",
    "        if not session and resource_uri.startswith(\"papers://\"):\n",
    "            for uri, sess in self.sessions.items():\n",
    "                if uri.startswith(\"papers://\"):\n",
    "                    session = sess\n",
    "                    break\n",
    "            \n",
    "        if not session:\n",
    "            print(f\"Resource '{resource_uri}' not found.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            result = await session.read_resource(uri=resource_uri)\n",
    "            if result and result.contents:\n",
    "                print(f\"\\nResource: {resource_uri}\")\n",
    "                print(\"Content:\")\n",
    "                print(result.contents[0].text)\n",
    "            else:\n",
    "                print(\"No content available.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    async def list_prompts(self):\n",
    "        \"\"\"List all available prompts.\"\"\"\n",
    "        if not self.available_prompts:\n",
    "            print(\"No prompts available.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nAvailable prompts:\")\n",
    "        for prompt in self.available_prompts:\n",
    "            print(f\"- {prompt['name']}: {prompt['description']}\")\n",
    "            if prompt['arguments']:\n",
    "                print(f\"  Arguments:\")\n",
    "                for arg in prompt['arguments']:\n",
    "                    arg_name = arg.name if hasattr(arg, 'name') else arg.get('name', '')\n",
    "                    print(f\"    - {arg_name}\")\n",
    "    \n",
    "    async def execute_prompt(self, prompt_name, args):\n",
    "        \"\"\"Execute a prompt with the given arguments.\"\"\"\n",
    "        session = self.sessions.get(prompt_name)\n",
    "        if not session:\n",
    "            print(f\"Prompt '{prompt_name}' not found.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            result = await session.get_prompt(prompt_name, arguments=args)\n",
    "            if result and result.messages:\n",
    "                prompt_content = result.messages[0].content\n",
    "                \n",
    "                # Extract text from content (handles different formats)\n",
    "                if isinstance(prompt_content, str):\n",
    "                    text = prompt_content\n",
    "                elif hasattr(prompt_content, 'text'):\n",
    "                    text = prompt_content.text\n",
    "                else:\n",
    "                    # Handle list of content items\n",
    "                    text = \" \".join(item.text if hasattr(item, 'text') else str(item) \n",
    "                                  for item in prompt_content)\n",
    "                \n",
    "                print(f\"\\nExecuting prompt '{prompt_name}'...\")\n",
    "                await self.process_query(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "    \n",
    "    async def chat_loop(self):\n",
    "        print(\"\\nMCP Chatbot Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "        print(\"Use @folders to see available topics\")\n",
    "        print(\"Use @<topic> to search papers in that topic\")\n",
    "        print(\"Use /prompts to list available prompts\")\n",
    "        print(\"Use /prompt <name> <arg1=value1> to execute a prompt\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "                if not query:\n",
    "                    continue\n",
    "        \n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "                \n",
    "                # Check for @resource syntax first\n",
    "                if query.startswith('@'):\n",
    "                    # Remove @ sign  \n",
    "                    topic = query[1:]\n",
    "                    if topic == \"folders\":\n",
    "                        resource_uri = \"papers://folders\"\n",
    "                    else:\n",
    "                        resource_uri = f\"papers://{topic}\"\n",
    "                    await self.get_resource(resource_uri)\n",
    "                    continue\n",
    "                \n",
    "                # Check for /command syntax\n",
    "                if query.startswith('/'):\n",
    "                    parts = query.split()\n",
    "                    command = parts[0].lower()\n",
    "                    \n",
    "                    if command == '/prompts':\n",
    "                        await self.list_prompts()\n",
    "                    elif command == '/prompt':\n",
    "                        if len(parts) < 2:\n",
    "                            print(\"Usage: /prompt <name> <arg1=value1> <arg2=value2>\")\n",
    "                            continue\n",
    "                        \n",
    "                        prompt_name = parts[1]\n",
    "                        args = {}\n",
    "                        \n",
    "                        # Parse arguments\n",
    "                        for arg in parts[2:]:\n",
    "                            if '=' in arg:\n",
    "                                key, value = arg.split('=', 1)\n",
    "                                args[key] = value\n",
    "                        \n",
    "                        await self.execute_prompt(prompt_name, args)\n",
    "                    else:\n",
    "                        print(f\"Unknown command: {command}\")\n",
    "                    continue\n",
    "                \n",
    "                await self.process_query(query)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "    \n",
    "    async def cleanup(self):\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    try:\n",
    "        await chatbot.connect_to_servers()\n",
    "        await chatbot.chat_loop()\n",
    "    finally:\n",
    "        await chatbot.cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f221f83-6e8b-431e-bd27-6dccedfac408",
   "metadata": {},
   "source": [
    "## Running the MCP Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7f73d-67fc-45bd-b418-31db8ee210ee",
   "metadata": {},
   "source": [
    "**Terminal Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c1087-f208-42df-a167-6de840046a58",
   "metadata": {},
   "source": [
    "- To open the terminal, run the cell below.\n",
    "- Open the terminal below.\n",
    "- Navigate to the `mcp_project` directory:\n",
    "    - `cd L7/mcp_project`\n",
    "- Activate the virtual environment:\n",
    "    - `source .venv/bin/activate`\n",
    "- Run the chatbot:\n",
    "    - `uv run mcp_chatbot.py`\n",
    "- To exit the chatbot, type `quit`.\n",
    "- If you run some queries and would like to access the `papers` folder or any output files: 1) click on the `File` option on the top menu of the notebook and 2) click on `Open` and then 3) click on `L7` -> `mcp_project`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486f985-1113-45ed-9d5a-fb7636e8a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new terminal\n",
    "import os\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(f\"{os.environ.get('DLAI_LOCAL_URL').format(port=8888)}terminals/4\", width=600, height=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21edb4-aba4-4b5a-b3c6-8aa908a7c8df",
   "metadata": {},
   "source": [
    "Make sure to interact with the chatbot. Here are some query examples:\n",
    "- **@folders**\n",
    "- **@ai_interpretability**\n",
    "- **/prompts**\n",
    "- **/prompt generate_search_prompt topic=history num_papers=2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cecbfb-764b-40ee-af41-9fd685dcda3a",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ðŸš¨\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc25a7ee-e500-44b0-ba6b-c339f52dc83e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> â¬‡ &nbsp; <b>To Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "<p> ðŸ“’ &nbsp; For more help, please see the <em>\"Appendix â€“ Tips, Help, and Download\"</em> Lesson.</p>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
